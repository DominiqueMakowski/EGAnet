{
    "collab_server" : "",
    "contents" : "#' Fit of Factor Model using Empirical Entropy\n#' @description Compute compute the entropy fit of a given structure.\n#' Lower values of the adjusted entropy fit and the adjusted entropy fit\n#' ranging from 0 to 1 suggest better fit with the data\n#'\n#' @param data A dataset\n#'\n#' @param structure A vector representing the structure (numbers or labels for each item).\n#' Can be theoretical factors or the structure detected by \\code{\\link{EGA}}\n#'\n#'\n#' @return Returns the values of entropy per factor, the mean empirical entropy (i.e., the average entropy across factors),\n#' the joint entropy (i.e. joint entropy for all factors), a modified empirical entropy per factor (that uses the frequencies of the sumscores),\n#' a mean modified empirical entropy. It also returns an adjusted entropy (ratio between the mean empirical entropy and\n#' the joint entropy) and an adjusted entropy varying from 0 to 1.\n#'\n#'\n#' @examples\n#' ega.wmt <- EGA(data = wmt2[,7:24], model = \"glasso\")\n#'\n#' entropyFit(data = wmt2[,7:24], structure = ega.wmt$wc)\n#'\n#' @seealso \\code{\\link{EGA}} to estimate the number of dimensions of an instrument using EGA and \\code{\\link{CFA}} to\n#' verify the fit of the structure suggested by EGA using confirmatory factor analysis.\n#'\n#' @author Hudson F. Golino <hfg9s at virginia.edu> and Alexander Christensen <alexpaulchristensen@gmail.com>\n#'\n#' @export\nentropyFit <- function (data, structure)\n{\n  require(plyr)\n\n  if(all(range(data)==c(0,1)))\n  {data <- ifelse(data==1,2,1)}\n\n  #convert structure to number if necessary\n  if(is.character(structure))\n  {\n    uni <- unique(structure)\n    num.comm <- structure\n\n    for(i in 1:length(uni))\n    {num.comm[which(num.comm==uniq[i])] <- i}\n\n  } else {num.comm <- structure}\n\n## Traditional Entropy:\n\n  #number of dimensions\n  n <- max(num.comm)\n  #communities sorted low to high\n  uniq <- sort(unique(num.comm))\n\n  #initialize entropy vector\n  H <- vector(\"numeric\",length=n)\n  bins <- floor(sqrt(nrow(data)/5))\n  seque <- matrix(NA,nrow=bins+1,ncol=n)\n  sums <- matrix(NA,nrow=nrow(data),ncol=n)\n  bin.sums <- vector(\"list\", n)\n  bin.sums2 <- matrix(NA, nrow=bins, ncol = n)\n  Freq <- matrix(NA,nrow=bins,ncol=n)\n\n  #compute empirical entropy for each community or item\n  for(i in 1:n)\n  {\n    if(n != ncol(data)){\n      sums[,i] <- rowSums(data[,which(num.comm==uniq[i])])\n    } else{\n        sums[,i] <- data[,i]\n      }\n    seque[,i] <- seq(from = range(sums[,i])[1], to = range(sums[,i])[2], length.out = bins+1)\n    bin.sums[[i]] <- table(cut(sums[,i], breaks = seque[,i], include.lowest = TRUE))\n    bin.sums2[,i] <- as.vector(unlist(bin.sums[[i]]))\n    Freq[,i] <- bin.sums2[,i]/sum(bin.sums2[,i])\n    H[i] <- -sum(ifelse(Freq[,i]>0,Freq[,i] * log(Freq[,i]),0))\n  }\n\n  # Joint Entropy:\n\n  bin.sums3 <- data.frame(matrix(NA, nrow = nrow(data), ncol = n))\n  joint.table <- vector(\"numeric\")\n  for(i in 1:n){\n    bin.sums3[,i] <- cut(sums[,i], breaks = seque[,i], include.lowest = TRUE)\n    joint.table = plyr::count(bin.sums3)$freq\n  }\n\n  freq.joint <- joint.table/sum(joint.table)\n  joint.entropy <- -sum(ifelse(freq.joint >0,freq.joint * log(freq.joint),0))\n\n\n  # Average Entropy:\n  items.com <- vector(\"numeric\")\n  for(i in 1:n){\n    items.com[[i]] <- sum(num.comm==uniq[i])\n  }\n  factor.average.h <- (sum(items.com*H)/length(num.comm))\n  h.watanabe <- factor.average.h-(ncol(data)*joint.entropy)\n  average.h <- factor.average.h-(joint.entropy)\n\n  # # Miller-Madow Bias Correction:\n  # # Individual Factors:\n  # non.zero.bins1 <- vector(\"numeric\",length=n)\n  # H.miller.madow <- vector(\"numeric\",length=n)\n  # for(i in 1:n){\n  #   non.zero.bins1[i] <- length(bin.sums2[bin.sums2[,i]!=0,i])\n  #   H.miller.madow[i] <- H[i]+((non.zero.bins1[i]-1)/(2*(nrow(data))))\n  # }\n\n  # Joint Entropy with Miller-Madow Bias Correction?\n\n\n  #compute mean emprirical entropy\n  #(empirical entropy per dimension)\n  ent <- mean(H)\n\n  result <- list()\n  result$Ind.Entropy <- H\n  result$Mean.Entropy <- ent\n  result$Joint.Entropy <- joint.entropy\n  result$Total.Correlation <- sum(H)-joint.entropy\n  result$Adj.Entropy <- ent-joint.entropy\n  result$Adj.Entropy2 <- exp(ent/joint.entropy)/(1+exp(ent/joint.entropy))\n  result$Adj.Entropy3 <- mean(H-joint.entropy)\n  result$Adj.Entropy4 <- 1-(exp(mean(H-joint.entropy))/(1+exp(mean(H-joint.entropy))))\n  result$Average.Entropy <- average.h\n  result$Factor.Average.Entropy <- factor.average.h\n  result$H.Watanabe <- h.watanabe\n  return(result)\n}\n#----\n",
    "created" : 1543499797756.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2179621689",
    "id" : "40D10918",
    "lastKnownWriteTime" : 1543506661,
    "last_content_update" : 1543506661563,
    "path" : "~/Dropbox/Pacotes do R/Pacote EGA/EGA/R/entropyFit.R",
    "project_path" : "R/entropyFit.R",
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}
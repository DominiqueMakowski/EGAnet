{
    "collab_server" : "",
    "contents" : "#load packages\nlibrary(entropy)\nlibrary(EGA)\nlibrary(NetworkToolbox)\nlibrary(lavaan)\nlibrary(psych)\nlibrary(Matrix)\n\nentropyModel <- function (data, comm)\n{\n  #convert comm to number if necessary\n  if(is.character(comm))\n  {\n    uni <- unique(comm)\n    num.comm <- comm\n\n    for(i in 1:length(uni))\n    {num.comm[which(num.comm==uniq[i])] <- i}\n\n  } else {num.comm <- comm}\n\n  #number of communities\n  n <- max(num.comm)\n  #communities sorted low to high\n  uniq <- sort(unique(num.comm))\n\n  #initialize entropy vector\n  H <- vector(\"numeric\",length=n)\n\n  #compute empirical entropy each community\n\n  # If the number of communities equals the number of variables,\n  # compute the entropy directly, column by column\n  if(!length(uniq)==ncol(data)){\n    for(i in 1:n)\n    {H[i] <- entropy::entropy.empirical(rowSums(data[,which(num.comm==uniq[i])]))}\n  } else {\n    H <- apply(data,2,entropy::entropy.empirical)\n  }\n\n\n  #compute mean emprirical entropy\n  #(empirical entropy per community)\n  ent <- mean(H)\n\n  result <- list()\n  result$ind <- H\n  result$full <- ent\n  return(result)\n}\n\n\nentropyCompare <- function(data, comm1, comm2)\n{\n  #minimum entropy\n  entropyMin <- function(data)\n  {\n    n <- ncol(data)\n\n    ent <- vector(\"numeric\",length=n)\n\n    for(i in 1:n)\n    {ent[i] <- entropy::entropy.empirical(data[,i])}\n\n    H <- mean(ent)\n\n    return(H)\n  }\n\n  #grab community object name\n  name1 <- deparse(substitute(comm1))\n  name2 <- deparse(substitute(comm2))\n\n  #unique number of communities\n  n1 <- length(unique(comm1))\n  n2 <- length(unique(comm2))\n\n  #minimum (each item as a factor)\n  Hmin <- entropyMin(data)\n\n  #maximum (one factor)\n  Hmax <- entropyModel(data, rep(1,ncol(data)))$full\n\n  #difference between min and max (variables - 1)\n  #variables minus 1 because we are excluding\n  #one-factor in the range of possible factor (2-48)\n  Hdiff <- (Hmax - Hmin)/(ncol(data)-1)\n\n  #estimate entropy\n  H1 <- entropyModel(data, comm1)$full\n  H2 <- entropyModel(data, comm2)$full\n\n  ##If not one-factor, then adjust\n  #if(n1!=1){\n    H1 <- H1 + exp(Hdiff*(n1-1))/(1+exp(Hdiff*(n1-1)))\n  #}else{H1 <- H1 + (exp(2*(n1-1)*Hdiff)/(1+exp(2*(n1+1)*Hdiff)))}\n\n  #If not one-factor, then adjust\n  #if(n2!=1){\n    H2 <- H2 +  exp(Hdiff*(n2-1))/(1+exp(Hdiff*(n2-1)))\n  #}else{H2 <- H2 + (exp(2*(n2-1)*Hdiff)/(1+exp(2+(n2+1)*Hdiff)))}\n\n  result <- as.data.frame(matrix(NA,nrow=2,ncol=2))\n\n  row.names(result) <- c(paste(name1),paste(name2))\n  colnames(result) <- c(\"H\",\"Lower\")\n\n  result$H <- c(H1,H2)\n  if(H1 < H2)\n  {result$Lower <- c(\"X\",\"\")\n  }else if(H1 > H2)\n  {result$Lower <- c(\"\",\"X\")\n  }else{result$Lower <- c(\"\",\"\")}\n  result$Hdiff <- Hdiff\n\n  return(result)\n}\n\n\nentropyModel2 <- function (data, comm)\n{\n  if(all(range(data)==c(0,1)))\n  {data <- ifelse(data==1,2,1)}\n\n  #convert comm to number if necessary\n  if(is.character(comm))\n  {\n    uni <- unique(comm)\n    num.comm <- comm\n\n    for(i in 1:length(uni))\n    {num.comm[which(num.comm==uniq[i])] <- i}\n\n  } else {num.comm <- comm}\n\n  #number of communities\n  n <- max(num.comm)\n  #communities sorted low to high\n  uniq <- sort(unique(num.comm))\n\n  #initialize entropy vector\n  H <- vector(\"numeric\",length=n)\n  sums <- matrix(NA,nrow=nrow(data),ncol=n)\n\n  #compute empirical entropy each community\n  for(i in 1:n)\n  {\n    sums[,i] <- rowSums(data[,which(num.comm==uniq[i])])\n    H[i] <- entropy::entropy.empirical(sums[,i])\n  }\n\n  #minimum entropy\n  entropyMin <- function(data)\n  {\n    var <- ncol(data)\n\n    Hm <- vector(\"numeric\",length=var)\n\n    for(i in 1:n)\n    {Hm[i] <- entropy::entropy.empirical(data[,i])}\n\n    Hmin <- mean(Hm)\n\n    return(Hmin)\n  }\n\n\n  #mutual information\n  if(n!=1)\n  {mi <- entropy::mi.empirical(sums)}\n\n  #compute mean emprirical entropy\n  #(empirical entropy per community)\n  ent <- mean(H)\n\n  #Difference between max and min entropy:\n\n  Hdiff <- (ent - Hmin)/(ncol(data)-1)\n\n\n\n\n  result <- list()\n  result$ind <- H\n  if(n!=1)\n  {\n    result$mut <- mi\n    result$full <- (ent + mi)\n  }else{result$full <- ent}\n\n  return(result)\n}\n\n\n\n#Example 1\ndat1 <- neoOpen\n#One-factor\no.comm1 = rep(1,48)\n#Theoretical\nt.comm1 = c(rep(1,8),rep(2,8),rep(3,8),rep(4,8),rep(5,8),rep(6,8))\n#Compare entropy\nentropyCompare(dat1,o.comm1,t.comm1) #Theoretical lower\nentropyFit(dat1, o.comm1)\nentropyFit(dat1, t.comm1)$Adj.Entropy\nvn.entropy(dat1, t.comm1)\n\n#TMFG\nega.tmfg1 <- EGA(neoOpen, model = \"TMFG\")\n#Compare\nentropyCompare(dat1,t.comm1,ega.tmfg1$wc) #TMFG lower\nentropyFit(dat1, t.comm1)\nentropyFit(dat1, ega.tmfg1$wc)$H.Watanabe\nvn.entropy(dat1, ega.tmfg1$wc)\nvn.entropy(dat1, t.comm1)\n\nentropyFit2(dat1, ega.tmfg1$wc)$Adj.Entropy[[1]]\n\n\n#glasso\nega.glas1 <- EGA(neoOpen, model = \"glasso\")\n#Compare\nentropyCompare(dat1,ega.glas1$wc,ega.tmfg1$wc) #glasso lower\nentropyFit(dat1, ega.glas1$wc)$H.Watanabe\nentropyFit(dat1, rep(1,48))\nvn.entropy(dat1, ega.glas1$wc)\n\nentropyFit2(dat1, ega.glas1$wc)$Adj.Entropy[[1]]\n\n\n\n\nsqrt(1)\n\n#glasso has lowest entropy per factor\n\ncfa.glas1 <- CFA(ega.glas1, estimator = \"WLSMV\", data = neoOpen)\ncfa.tmfg1 <- CFA(ega.tmfg1, estimator = \"WLSMV\", data = neoOpen)\n\nlavTestLRT(cfa.glas1$fit,cfa.tmfg1$fit,method=\"satorra.bentler.2001\")\n#sig., suggesting the more complex model (glasso)\n\n\n#Example2\ndat2 <- wmt2[,7:24]\n#One-factor entropy\no.comm2 = rep(1,18)\n#TMFG entropy\nega.tmfg2 <- EGA(dat2, model = \"TMFG\")\n#Compare\nentropyCompare(dat2,o.comm2,ega.tmfg2$wc) #TMFG lower\nentropyFit(dat2, ega.tmfg2$wc)\nentropyFit(dat2, o.comm2)\nvn.entropy(dat1, ega.tmfg2$wc)\nentropyFit2(dat2, ega.tmfg2$wc)$Adj.Entropy[[1]]\n\n\n#glasso entropy\nega.glas2 <- EGA(dat2, model = \"glasso\")\n#Compare\nentropyCompare(dat2,ega.glas2$wc,ega.tmfg2$wc) #TMFG lower\nentropyFit(dat2, ega.glas2$wc)\nvn.entropy(dat1, ega.glas2$wc)\n\n\nentropyFit2(dat2, ega.glas2$wc)$Adj.Entropy[[1]]\n\n\n#Compare one-factor and glasso\nentropyCompare(dat2,ega.glas2$wc,o.comm2) # glasso lower\n# Consistent with CFA\n\nmod2 <- 'one =~\nwmt1 + wmt2 + wmt3 + wmt4 + wmt5 + wmt6 + wmt7 + wmt8 + wmt9 +\nwmt10 + wmt11 + wmt12 + wmt13 + wmt14 + wmt15 + wmt16 + wmt17 + wmt18\n'\n\n#TMFG has lowest entropy per factor\n\ncfa.glas2 <- CFA(ega.glas2, estimator = \"WLSMV\", data = dat2)\ncfa.tmfg2 <- CFA(ega.tmfg2, estimator = \"WLSMV\", data = dat2)\ncfa.mod2 <- cfa(mod2, estimator = \"WLSMV\", data = dat2)\nlibrary(lavaan)\nfitMeasures(cfa.mod2)\n\nlavTestLRT(cfa.mod2,cfa.glas2$fit,cfa.tmfg2$fit,method=\"satorra.bentler.2001\")\n\nlavTestLRT(cfa.glas2$fit,cfa.mod2,method=\"satorra.bentler.2001\")\n\n\n#sig., suggesting the more complex model (TMFG)\n# glasso fits better than one-factor\n\n#Example3\n#One-factor entropy]\ndat3 <- na.omit(bfi[,1:25])\no.comm3 = rep(1,25)\n#Theoretical entropy\nt.comm3 = c(rep(1,5),rep(2,5),rep(3,5),rep(4,5),rep(5,5))\n#Compare\nentropyCompare(dat3,o.comm3,t.comm3) #theoretical lower\nentropyFit(dat3, t.comm3)\nentropyFit(dat3, o.comm3)\nvn.entropy(dat3, t.comm3)\n\n\n#TMFG entropy\nega.tmfg3 <- EGA(dat3, model = \"TMFG\")\n#Compare\nentropyCompare(dat3,ega.tmfg3$wc,t.comm3) #Theoretical lower\nentropyFit(dat3, ega.tmfg3$wc)\nvn.entropy(dat3, ega.tmfg3$wc)\n\n\n#glasso entropy\nega.glas3 <- EGA(dat3, model = \"glasso\")\n#Compare\nentropyCompare(dat3,o.comm3,ega.glas3$wc) #equal\nentropyFit(dat3, ega.glas3$wc)\nvn.entropy(dat3, ega.glas3$wc)\n\n\nentropyCompare(dat3,ega.tmfg3$wc,ega.glas3$wc) #Glasso lower\n\n\n#suggests Glasso is lower\n\ncfa.glas3 <- CFA(ega.glas3, estimator = \"WLSMV\", data = dat3)\ncfa.tmfg3 <- CFA(ega.tmfg3, estimator = \"WLSMV\", data = dat3)\n\nlavTestLRT(cfa.glas3$fit,cfa.tmfg3$fit,method=\"satorra.bentler.2001\")\n#sig., suggesting the more complex model (glasso/theoretical)\n# CONSISTENT WITH ENTROPY\n\n\n#Example 4\nf2 <-as.matrix(bdiag(rep(0.9, 30)))\nsim.onef <- sim.structure(fx = f2, n = 5000, raw = TRUE, items = TRUE, cat = 1, low = -0.5, high = 0.5)\ndat4 <- as.data.frame(sim.onef$observed)\n\n\n#FIT CHANGES WITH EACH SAMPLE GENERATED\n\n\n#One-factor entropy\no.comm4 = rep(1,30)\no.comm5 = rep(1:2,15)\no.comm6 = rep(1:3,10)\no.comm7 = rep(1:5,6)\no.comm8 = rep(1:10,3)\no.comm9 = rep(1:15,2)\n\n#TMFG\nega.tmfg4 <- EGA(dat4, model = \"TMFG\")\n#Compare\nentropyCompare(dat4,o.comm4,ega.tmfg4$wc)\nentropyFit(dat4, ega.tmfg4$wc)\n\nentropyFit(dat4, o.comm4)\n\n\n#glasso\nega.glas4 <- EGA(dat4, model = \"glasso\")\nentropyCompare(dat4,o.comm4,ega.glas4$wc)\n\nentropyCompare(dat4,ega.tmfg4$wc,ega.glas4$wc)\n\n\n#Compare TMFG and glasso\nentropyCompare(dat4,o.comm4, o.comm9)\n\nentropyModel(dat4,ega.glas4$wc)$full\nentropyModel(dat4,o.comm)$full\n\n?exp\n\nexpm1(1/4)\n\n\ncfa.tmfg4 <- CFA(ega.tmfg4, estimator = \"WLSMV\", data = dat4)\ncfa.glas4 <- CFA(ega.glas4, estimator = \"WLSMV\", data = dat4)\n\nmod4 <- '\none =~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 +\nV11 + V12 + V13 + V14 + V15 + V16\n'\ncfa4 <- cfa(mod4,estimator=\"WLSMV\", data = dat4)\n\nlavTestLRT(cfa.glas4$fit,cfa.tmfg4$fit,cfa4,method=\"satorra.bentler.2001\")\nlavTestLRT(cfa.glas4$fit,cfa4,method=\"satorra.bentler.2001\")\nlavTestLRT(cfa.glas4$fit,cfa.tmfg4$fit,method=\"satorra.bentler.2001\")\n\n#sig., suggesting the more complex model (glasso/theoretical)\n\nmean(apply(dat4,2,entropy::entropy.empirical))\n\nlog2(2)\nx <- 1:20\n\nexp(1)/exp(2)\n\n\n\nxlog <- (exp(0.003613972*(x-1)))/(1+(exp(0.003613972*(x-1))))\nplot(xlog)\n\nexp(2)/2^3\n?exp\n\n-3 +3.5\nexp(2)/(1+exp(2))\n",
    "created" : 1542828204611.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3519691812",
    "id" : "CDBD42E1",
    "lastKnownWriteTime" : 1543507321,
    "last_content_update" : 1543507321495,
    "path" : "~/Dropbox/Pacotes do R/Pacote EGA/New Functions to be Added/entropy_fit_compare.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}